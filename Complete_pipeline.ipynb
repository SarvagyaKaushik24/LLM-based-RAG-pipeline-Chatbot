{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "\n",
    "import torch \n",
    "import requests \n",
    "import fitz\n",
    "from tqdm import tqdm \n",
    "import numpy as np \n",
    "import textwrap\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_formatter(text: str) -> str : \n",
    "    clean_txt = text.replace(\"\\n\",\" \").strip()\n",
    "    return clean_txt\n",
    "\n",
    "\n",
    "#     return pages_and_texts\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_path)  # open a document\n",
    "    pages_and_texts = []\n",
    "    print(len(doc))\n",
    "    n = len(doc)\n",
    "    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n",
    "        if page_number <= n :\n",
    "            text = page.get_text()  # get plain text encoded as UTF-8\n",
    "            text = text_formatter(text)\n",
    "            pages_and_texts.append({\"page_number\": page_number,  \n",
    "                                    \"page_char_count\": len(text),\n",
    "                                    \"page_word_count\": len(text.split(\" \")),\n",
    "                                    \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                    \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                                    \"text\": text})\n",
    "    return pages_and_texts\n",
    "\n",
    "def chunking(input_list , chunk_size) :\n",
    "    l = [input_list[i : i+ chunk_size] for i in range(0,len(input_list), chunk_size)]\n",
    "    return l\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    \"\"\"Wrap and print text with a given line width.\"\"\"\n",
    "    # Ensure text is a string\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)  # Convert list to string\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)\n",
    "\n",
    "def search_similar_sentences(query_sentence,model,index, k=5):\n",
    "    # Generate embedding for the query sentence\n",
    "    query_embedding = model.encode([query_sentence], convert_to_numpy=True)\n",
    "    \n",
    "    # Ensure query_embedding is 2D\n",
    "    if query_embedding.ndim == 1:\n",
    "        query_embedding = np.expand_dims(query_embedding, axis=0)\n",
    "    \n",
    "    # Search the index\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    return distances, indices\n",
    "\n",
    "def print_top_k_results(query,k,distances,indices):\n",
    "    for i in range(k): \n",
    "        print(f\"Distance: {distances[0][i]}\")\n",
    "        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "        print(\"Text:\")\n",
    "        print_wrapped(pages_and_chunks[indices[0][i]][\"chunks\"])\n",
    "        # Print the page number too so we can reference the textbook further (and check the results)\n",
    "        print(f\"Page number: {pages_and_chunks[indices[0][i]]['page_number']}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "def prompt_formatter(query, context_items, tokenizer, use_dialogue_template=True):\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    # Join context items into one dotted paragraph\n",
    "    # context = \"- \" + \"\\n- \".join([item[\"chunks\"] for item in context_items])\n",
    "    context = \" \".join([item[\"chunks\"] for item in context_items])\n",
    "\n",
    "    # Create a base prompt with examples to help the model\n",
    "    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
    "    # We could also write this in a txt file and import it in if we wanted.\n",
    "    base_prompt = \"\"\"\n",
    "        Based on the following context items, please answer the query.\n",
    "        Context item : \n",
    "        {context}\n",
    "        User query: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "\n",
    "    # Update base prompt with context items and query   \n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    if(use_dialogue_template == True) :\n",
    "        # Apply the chat template\n",
    "        prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True)\n",
    "    else : \n",
    "        prompt = tokenizer.apply_chat_template(conversation=base_prompt,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True) \n",
    "    return prompt\n",
    "\n",
    "def ask(query,\n",
    "        model,\n",
    "        index,\n",
    "        pages_and_chunks,\n",
    "        tokenizer,\n",
    "        llm,\n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True, \n",
    "        return_answer_only=True,\n",
    "        use_cache=False):\n",
    "    \"\"\"\n",
    "    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get just the scores and indices of top related results\n",
    "    # Get relevant resources\n",
    "    scores, indices = search_similar_sentences(query,model,index)\n",
    "        \n",
    "    # Create a list of context items\n",
    "    context_items = [pages_and_chunks[i] for i in indices[0]]\n",
    "    # context_items=[]\n",
    "\n",
    "    # Add score to context item\n",
    "    for i, item in enumerate(context_items):\n",
    "        item[\"score\"] = scores[0][i] # return score back to CPU \n",
    "        \n",
    "    # Format the prompt with context items\n",
    "    prompt = prompt_formatter(query=query,\n",
    "                              context_items=context_items,tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"model name\")\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # # Generate an output of tokens\n",
    "    # outputs = llm.generate(**input_ids,\n",
    "    #                              temperature=temperature,\n",
    "    #                              do_sample=True,\n",
    "    #                              max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    # # Turn the output tokens into text\n",
    "    # output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    # if format_answer_text:\n",
    "    #     # Replace special tokens and unnecessary help message\n",
    "    #     output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
    "\n",
    "    # # Only return the answer without the context items\n",
    "    # if return_answer_only:\n",
    "    #     return output_text\n",
    "    \n",
    "    # return output_text, context_items\n",
    "    outputs = llm.generate(**input_ids,\n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=True,\n",
    "                                 max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    # Turn the output tokens into text\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    if format_answer_text:\n",
    "        # Replace special tokens and unnecessary help message\n",
    "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
    "\n",
    "    # Only return the answer without the context items\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "    \n",
    "    return output_text, context_items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/ashhar21137/anaconda3/envs/ashhar_env2/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/raid/home/ashhar21137/anaconda3/envs/ashhar_env2/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435f267ec8d34163896064e2ea6b9ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 54.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAlibaba-NLP/gte-Qwen2-1.5B-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# In case you want to reduce the maximum length:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mmax_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8192\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ashhar_env2/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:316\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data)\u001b[0m\n\u001b[1;32m    312\u001b[0m     modules \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;28mstr\u001b[39m(idx), module) \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules)])\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(modules)\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_hpu_graph_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts:\n",
      "File \u001b[0;32m~/anaconda3/envs/ashhar_env2/lib/python3.10/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ashhar_env2/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ashhar_env2/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 779 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/ashhar_env2/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ashhar_env2/lib/python3.10/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ashhar_env2/lib/python3.10/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 54.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", trust_remote_code=True)\n",
    "# In case you want to reduce the maximum length:\n",
    "model.max_seq_length = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AutoModelForCausalLM.from_pretrained( \n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",  \n",
    "    device_map=\"cuda\",  \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,  \n",
    ") \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import re\n",
    "import faiss\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "def process_pdf_and_answer_query(pdf_path, query,model,tokenizer,llm):\n",
    "    # Extract text from the PDF\n",
    "    # Split the file path into root and extension\n",
    "    file_name, file_extension = os.path.splitext(pdf_path)\n",
    "\n",
    "    # Save the file name (without extension) in file_name variable\n",
    "    file_name = file_name\n",
    "\n",
    "    pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "\n",
    "    df = pd.DataFrame(pages_and_texts)\n",
    "    chunk_size=10\n",
    "\n",
    "    \n",
    "    for item in tqdm(pages_and_texts) : \n",
    "        text = item['text']\n",
    "        item[\"sentences\"] = nltk.tokenize.sent_tokenize(text, language='english') \n",
    "\n",
    "        item['page_sentence_count_nltk'] = len(item['sentences'])\n",
    "    \n",
    "    for item in tqdm(pages_and_texts) : \n",
    "        item[\"chunks\"] = chunking(item['sentences'], chunk_size)\n",
    "        item['num_chunks'] = len(item[\"chunks\"])\n",
    "\n",
    "\n",
    "    # Split each chunk into its own item\n",
    "    pages_and_chunks = []\n",
    "    for item in tqdm(pages_and_texts):\n",
    "        for chunk in item[\"chunks\"]:\n",
    "            chunk_dict = {}\n",
    "            chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "            \n",
    "            # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "            joined_sentence_chunk = \"\".join(chunk).replace(\"  \", \" \").strip()\n",
    "            joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "            chunk_dict[\"chunks\"] = joined_sentence_chunk\n",
    "\n",
    "            # Get stats about the chunk\n",
    "            chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "            chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "            chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "        \n",
    "            pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "    df = pd.DataFrame(pages_and_chunks)\n",
    "    min_token_len = 20\n",
    "    df[df[\"chunk_token_count\"] <= min_token_len][\"chunks\"]\n",
    "\n",
    "    # for row in df[df[\"chunk_token_count\"] <= min_token_len].sample(1).iterrows(): \n",
    "    #     print(f'CHunk token count : {row[1][\"chunk_token_count\"]} | text : {row[1][\"chunks\"]}')\n",
    "\n",
    "    pages_and_chunks_over_threshold = df[df[\"chunk_token_count\"] > min_token_len].to_dict(orient=\"records\")\n",
    "\n",
    "    text_chunks = [item[\"chunks\"] for item in pages_and_chunks_over_threshold]\n",
    "\n",
    "\n",
    "    embedding_dim = model.get_sentence_embedding_dimension()\n",
    "    # print(embedding_dim)\n",
    "\n",
    "    index_file_path = f'{file_name}.index'\n",
    "\n",
    "    if not os.path.isfile(index_file_path):\n",
    "        index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "        # Create embeddings one by one on the GPU and add to FAISS index\n",
    "        for item in tqdm(pages_and_chunks_over_threshold):\n",
    "            embeddings = model.encode(item[\"chunks\"], batch_size=32, convert_to_numpy=True)\n",
    "            item[\"embedding\"] = embeddings\n",
    "            # print(embeddings.shape)\n",
    "            if embeddings.ndim == 1:\n",
    "                # If embeddings is 1D, reshape it to 2D\n",
    "                embeddings = np.expand_dims(embeddings, axis=0)\n",
    "            elif embeddings.ndim != 2:\n",
    "                raise ValueError(\"Embeddings should be a 2D array with shape (num_chunks, embedding_dim).\")\n",
    "            index.add(embeddings)\n",
    "\n",
    "        faiss.write_index(index, f'{file_name}.index')\n",
    "\n",
    "    else:\n",
    "      print(\"File already exists. Reading from it\")\n",
    "      index = faiss.read_index(index_file_path)\n",
    "      \n",
    "    # Use the NLP model to answer the query\n",
    "    answer,_=ask(query=query,\n",
    "                            model=model,\n",
    "                            index=index,\n",
    "                            pages_and_chunks=pages_and_chunks,\n",
    "                            tokenizer=tokenizer,\n",
    "                            llm=llm,\n",
    "                            temperature=0.7,\n",
    "                            max_new_tokens=512,\n",
    "                            return_answer_only=False)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_path = 'pink_2.pdf'\n",
    "# query = \"What was the amount received by ASP Sarin reality pvt ltd by its holding company Supertech ltd?\"\n",
    "\n",
    "# answer=process_pdf_and_answer_query(pdf_path,query,model,tokenizer,llm)\n",
    "# print(f\"Answer:\\n\")\n",
    "# print_wrapped(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "# def wrapped_process_pdf_and_answer_query(pdf_file, query):\n",
    "#     return process_pdf_and_answer_query(pdf_file, query, model, tokenizer, llm)\n",
    "\n",
    "\n",
    "# # Define the Gradio interface\n",
    "# iface = gr.Interface(\n",
    "#     fn=wrapped_process_pdf_and_answer_query,\n",
    "#     inputs=[\n",
    "#         gr.File(label=\"Upload PDF\"),\n",
    "#         gr.Textbox(label=\"Enter your query\")\n",
    "        \n",
    "#     ],\n",
    "#     outputs=\"text\",\n",
    "#     title=\"PDF Query Answering\",\n",
    "#     description=\"Upload a PDF and ask a question about its content.\"\n",
    "# )\n",
    "\n",
    "# # Launch the Gradio interface\n",
    "# iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Mock functions and variables for demonstration purposes\n",
    "# Replace these with actual implementations\n",
    "existing_pdfs = [\"short_stories.pdf\", \"WiFiTuned_TOCHI.pdf\", \"os.pdf\"]  # List of existing PDFs\n",
    "\n",
    "# def process_pdf_and_answer_query(pdf_content, query, model, tokenizer, llm):\n",
    "#     # Placeholder function - replace with actual implementation\n",
    "#     return f\"Processed the query: '{query}' on the provided PDF.\"\n",
    "\n",
    "def wrapped_process_pdf_and_answer_query(pdf_choice, pdf_file, query):\n",
    "    if pdf_choice:\n",
    "        pdf_content = pdf_choice\n",
    "    elif pdf_file is not None:\n",
    "        pdf_content = pdf_file.name\n",
    "    else:\n",
    "        return \"Please provide a PDF file.\"\n",
    "\n",
    "    return process_pdf_and_answer_query(pdf_content, query, model, tokenizer, llm)\n",
    "\n",
    "# Define the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=wrapped_process_pdf_and_answer_query,\n",
    "    inputs=[\n",
    "        gr.Dropdown(choices=existing_pdfs, label=\"Choose from existing PDFs\"),\n",
    "        gr.File(label=\"Or upload your own PDF\"),\n",
    "        gr.Textbox(label=\"Enter your query\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"PDF Query Answering\",\n",
    "    description=\"Choose an existing PDF or upload your own, and ask a question about its content.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ashhar_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
